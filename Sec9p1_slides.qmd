---
title: "9.1 $z$ Tests and Confidence Intervals for a Difference Between Two Population Means"
subtitle: "Mathematical Statistics"
author: "MATH 365"
format:
  revealjs:
    theme: [default, custom.scss]
    incremental: false
    transition: fade
    chalkboard: true
    smaller: true
    pdf-export: true
    toc: false
    toc-depth: 1
    reveal-options:
      slideNumber: true
    html-math-method: mathjax
    preview-links: auto
    notes: true
title-slide-attributes:
  data-background-color: "#EA6820"
from: markdown+emoji
execute:
  echo: true
  cache: false
  eval: false
editor:
  markdown:
    wrap: 72
---

### Topics for Today :dart:

-   Sampling distribution of differences in means
-   $z$-Test procedures for known population variances
-   Causality vs association: an example
-   Type II error and power ($\beta$)
-   Large-sample methods
-   Confidence intervals for $\mu_1 - \mu_2$

------------------------------------------------------------------------

## Review from Last Time :monocle_face:

**Recall:**

-   How do we distinguish practical vs. statistical significance?
-   What concerns arise with multiple testing?

------------------------------------------------------------------------

### Do stats students with early classes get less sleep than those with later classes?

The data:

```{r}
sleepdata <- read.csv(file = "http://people.kzoo.edu/enordmoe/math365/data/sleepdata2samp.csv")
```

<br>

::: fragment
:memo: Plot the data and get summary statistics
:::

------------------------------------------------------------------------

## Sampling Distribution of $\bar{X} - \bar{Y}$

**Conditions:**

::: incremental
-   Independent random samples
-   Known $\sigma_1^2$, $\sigma_2^2$
-   Normal populations
:::

::: fragment
$$
Z =
\frac{\bar{X} - \bar{Y} - (\mu_1 - \mu_2)}{\sqrt{\frac{\sigma_1^2}{m} + \frac{\sigma_2^2}{n}}}
$$

:brain: **Question:** Why do we assume independence between samples?
:::

------------------------------------------------------------------------

### $z$-Test Procedures with Known Variances {.smaller}

**Hypotheses:**

-   $H_0: \mu_1 - \mu_2 = \Delta_0$
-   $H_a$: one-sided or two-sided depending on context

::: fragment
**Test Statistic:**

$$
z = \frac{\bar{x} - \bar{y} - \Delta_0}{\sqrt{\frac{\sigma_1^2}{m} + \frac{\sigma_2^2}{n}}}
$$
:::

::: fragment
**P-values:**

-   Upper-tailed: $P(Z \ge z)$
-   Lower-tailed: $P(Z \le z)$
-   Two-tailed: $2 P(Z \ge |z|)$
:::

::: fragment
:hammer_and_wrench: Use `pnorm()` to compute $p$-values as before
:::

------------------------------------------------------------------------

### Example: Medical Academics vs Practitioners {.smaller}

:test_tube: A published study reports number of years from graduation to
death of 215 male physicians (why only males?) :

-   Full-time practice MDs : $\bar{x}_1 = 48.9$, $n_1 = 125$,
    $\sigma_1 = 14.6$

-   Academic MDs: $\bar{x}_2 = 43.2$, $n_2 = 90$, $\sigma_2 = 14.4$

Is the difference statistically discernible?

::: fragment
Test $H_0: \mu_1-\mu_2=0$ versus $H_a:\mu_1-\mu_2\neq 0$.
:::

::: fragment
Compute the $z$-test statistic:

$$
z = \frac{48.9 - 43.2}{\sqrt{\frac{14.6^2}{125} + \frac{14.4^2}{90}}}
\approx 2.85
$$
:::

::: fragment
$p=2 P(Z\ge 2.85)= .0044$ so we have strong evidence of a difference.
:::

::: fragment
> Does this imply causation?
:::

------------------------------------------------------------------------

## Publish **and** Perish? :exploding_head:

::: fragment
:mag: **Causality Considerations:**

-   Observational data $\neq$ random assignment

-   Possible confounding: career path, health history, stress levels

-   Reverse causation?
:::

::: fragment
:point_right: Retrospective study: use caution in interpretation
:::

------------------------------------------------------------------------

## Power and Type II Error: $\beta$ {.smaller}

**Definitions:**

-   Type II error: failing to reject $H_0$ when $H_a$ is true

![](images/beta_2samp.png){fig-align="center"}

::: fragment
-   Power: $1 - \beta$\
:::

------------------------------------------------------------------------

## :bulb: Sample Size for Desired $\alpha$ and $\beta$

**One-sided test with equal sample sizes:**

$$
m = n = \frac{(\sigma_1^2 + \sigma_2^2)(z_\alpha + z_\beta)^2}{(\Delta' - \Delta_0)^2}
$$

::: fragment
Replace $\alpha$ by $\alpha/2$ for a two-tailed test.
:::

------------------------------------------------------------------------

## Find sample sizes for sleep data comparison

Assume $\sigma_1=\sigma_2=1.5$ and $\alpha=.05$ and $\beta = 0.20$. For
a two-tailed test:

$$
m = n = \frac{(\sigma_1^2 + \sigma_2^2)(z_{\alpha/2} + z_\beta)^2}{(\Delta' - \Delta_0)^2}
$$

------------------------------------------------------------------------

## Large Sample $z$ Tests {.smaller}

When $m,n$ large ($\ge 40$), CLT implies normality:

$$
Z =
\frac{\bar{X} - \bar{Y} - \Delta_0}{\sqrt{\frac{S_1^2}{m} + \frac{S_2^2}{n}}}
$$

:white_check_mark: Use even when population distributions are not normal

------------------------------------------------------------------------

## Large-Sample Confidence Intervals {.smaller}

When $m,n$ are large:

$$
(\bar{X} - \bar{Y}) \pm z_{\alpha/2}
\sqrt{\frac{S_1^2}{m} + \frac{S_2^2}{n}} 
$$

:mag: Width of interval depends on:

-   Sample sizes

-   Variability ($S^2$ values)

-   Confidence level

------------------------------------------------------------------------

## :airplane: Flight Delays: UA vs AA {.smaller}

The `FlightDelays` dataset from the `resampledata3` package contains
flight delay info for American and United Airlines.

Explore delay times for two airlines: UA (United) and AA (American)
using this code chunk:

```{r}
# Install the package if necessary
if (!requireNamespace("resampledata3", quietly = TRUE)) {
  install.packages("resampledata3")
}
library(resampledata3)

# Summary statistics
aggregate(Delay ~ Carrier, data = FlightDelays, function(x) c(mean = mean(x), sd = sd(x), n=length(x)))
# Boxplot
boxplot(Delay ~ Carrier, data = FlightDelays, horizontal = TRUE,
        main = "Delays: UA vs AA", ylab = "Delay (min)")
```

------------------------------------------------------------------------

## :airplane: Flight Delays Carry out $Z$ hypothesis test and confidence interval procedures



# Investigating Properties of Two-Sample Normal Tests

------------------------------------------------------------------------

### How well do these procedures actually work?  

:::{.incremental}  

-   **Confidence interval:** How often does a 95% CI for $\mu_1 - \mu_2$ contain the true
    difference?
-   **Hypothesis Test:**  What proportion of $\alpha$-level tests reject $H_0$ when it's true?

:::  


## CI Coverage & Type I Error Investigation  

Use the following R function to investigate properties assuming the normality condition holds.


```{r}
sim_diff_means <- function(n = 30, mu1 = 5, mu2 = 5, sigma1 = 1, sigma2 = 1, reps = 10000) {
  count_in_ci <- 0
  type1_errors <- 0
  for (i in 1:reps) {
    x <- rnorm(n, mu1, sigma1)
    y <- rnorm(n, mu2, sigma2)
    est <- mean(x) - mean(y)
    se <- sqrt(var(x)/n + var(y)/n)
    ci <- est + c(-1,1)*qnorm(0.975)*se
    if (ci[1] <= 0 && ci[2] >= 0) count_in_ci <- count_in_ci + 1
    z <- est / se
    pval <- 2*(1 - pnorm(abs(z)))
    if (pval < 0.05) type1_errors <- type1_errors + 1
  }
  list(CI_coverage = count_in_ci / reps, Type1_error = type1_errors / reps)
}
sim_diff_means()
```

::: callout-tip
This function could also be used in  modified form to investigate properties of large sample
$z$ procedures or (later) $t$-procedures.
:::

------------------------------------------------------------------------



## Summary: Section 9.1 :white_check_mark:

-   $z$ tests apply with known variances or large $n$
-   Hypothesis tests and CIs require conditions: independence, normality
    (or large $n$)
-   Causality $\neq$ statistical significance
-   Power and Type II error depend on $\Delta$ and $\sigma$
-   Simulation in R confirms the theory!

:star2: Next: inference for Normal data with unknown variances (back to the $t$!)