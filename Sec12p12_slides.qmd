---
title: "12.1 and 12.2: <br> The Simple Linear Model and Parameter Estimation"
subtitle: "Mathematical Statistics"
author: "MATH 365"
format:
  revealjs:
    theme: [default, custom.scss]
    incremental: false
    transition: fade
    chalkboard: true
    smaller: true
    pdf-export: true
    toc: false
    toc-depth: 1
    reveal-options:
      slideNumber: true
    html-math-method: mathjax
    preview-links: auto
    notes: true
title-slide-attributes:
  data-background-color: "#EA6820"
from: markdown+emoji
execute:
  echo: true
  cache: false
  eval: true
editor:
  markdown:
    wrap: 72
---

## Topics for Today

-   :bar_chart: Simple Linear Regression Model
-   :1234: Least Squares Estimation of Parameters
-   :abacus: Estimating Variance and $R^2$
-   :chart_with_upwards_trend: Visualizing data and interpreting output
-   :computer: Regression in R with `lm()`, `summary()`, and `anova()`

> **Motivating Question:** How do we model the relationship between two
> variables and make predictions using observed data?

------------------------------------------------------------------------

## Modeling the Effect of Advertising on Sales

> :bulb: A classic question in business and economics:

> **"Did the advertising campaign increase sales?"**

Firms invest heavily in advertising. They want evidence that their
spending leads to measurable gains.

------------------------------------------------------------------------

## So How Do You Answer That?

-   You have **data**: weekly ad spending, sales revenue, market
    conditions.
-   You suspect: more advertising → higher sales.
-   But: sales fluctuate due to many factors — holidays, pricing,
    competitors, random noise...

. . .

> :question: **How do we isolate the effect of advertising from
> everything else?**

. . .

-   Can we **quantify** the average change in sales per \$1000 spent?
-   Can we separate **signal from noise**?
-   Can we **predict** sales based on ad spending?

------------------------------------------------------------------------

## The Role of a Statistical Model

> A **statistical model** gives us a way to describe and estimate
> relationships between variables.

We assume: $$
\text{Sales} = f(\text{Ad Spending}) + \varepsilon
$$

Where:\
- $f(\cdot)$ is the **systematic trend** we want to estimate\
- $\varepsilon$ is the **random variation** due to unmeasured or
uncontrollable factors

------------------------------------------------------------------------

## The Key Idea of Regression {.smaller}

::: callout-note
"The key idea in developing a probabilistic relationship between a
**dependent** or **response** variable $y$ and an **independent**,
**explanatory**, or **predictor** variable $x$ is to realize that once
the value of $x$ has been fixed, there is *still* uncertainty in what
the resulting $y$ value will be.” (Devore & Berk, 2012)
:::

We seek to characterize the *stochastic dependence* of the random
variable $Y$ given $x$.

**Model:**

$$
Y = f(x) + \epsilon
$$

------------------------------------------------------------------------

## The Simple Linear Model

We focus on **linear models** of the form:

$$
Y = \beta_0 + \beta_1 x + \epsilon
$$

::: callout-important
Where:\
- $\epsilon \sim N(0, \sigma^2)$\
- $\epsilon$'s are independent\
- Each $(x_i, y_i)$ pair is a realization from this model
:::

We aim to estimate $\beta_0$, $\beta_1$, and $\sigma^2$ from the data.

------------------------------------------------------------------------

## A Linear Probabilistic Model

Instead of a deterministic relationship, we assume:

> The **expected value** of $Y$ is a linear function of $x$:

$$
E(Y | x) = \beta_0 + \beta_1 x
$$

And the observed $Y$ deviates from this expected value due to random
error $\epsilon$.

------------------------------------------------------------------------

## Visualizing the Simple Linear Regression Model

![Devore Fig
12.3](images/regr_true_plot.png){fig-alt="scatterplot of true model"
fig-align="center"}

------------------------------------------------------------------------

## Estimating Model Parameters

Use the **method of least squares** to find the coefficients $\widehat\beta_0$ and $\widehat\beta_1$ that minimize the sum of squared errors:

$$
f(b_0, b_1)= \sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2
$$

This yields the **estimated regression line**:

$$
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x
$$

---

## Least Squares Estimates

::: callout-note
**Formulas:**

$$
\hat{\beta}_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2} = \frac{S_{xy}}{S_{xx}}, \quad \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
$$
:::

<br>

Under the assumption $\epsilon \sim N(0, \sigma^2)$, these are also
**maximum likelihood estimates**.

---

## Example: Swirling Time vs Distance

:test_tube: From the student funnel lab: use `distance` to predict
`tswirl`.

**Variables:**   
- $x$ = distance up the channel (cm)   
- $y$ = swirling time (sec)    
  

```r
funneldata <- read.csv("http://people.kzoo.edu/enordmoe/math365/data/funneldata.csv")
lm_fit <- lm(tswirl ~ distance, data = funneldata)
summary(lm_fit)
plot(tswirl ~ distance, data = funneldata)
abline(lm_fit)
```

------------------------------------------------------------------------

## Interpreting Output in R

``` r
summary(lm_fit)
```

Includes:  
- Estimates $\hat{\beta}_0$, $\hat{\beta}_1$   
- Standard errors  
- t-values and p-values   
- $R^2$

``` r
anova(lm_fit)
```

Used to test overall model fit (F-test).

---  

### R Regression Output Example {.smaller}

`summary()` output:  

```{r}
#| echo: false
funneldata <- read.csv("http://people.kzoo.edu/enordmoe/math365/data/funneldata.csv")
lm_fit <- lm(tswirl ~ distance, data = funneldata)
summary(lm_fit)
```
`anova()` output:  

```{r}
#| echo: false
anova(lm_fit)
```



------------------------------------------------------------------------

## Example: Purrfect Prediction? :feet:

:cat: Predict `weight` of cats based on `height` using `MASS::cats`

``` r
library(MASS)
lm_cats <- lm(Bwt ~ Hwt, data = cats)
summary(lm_cats)
plot(Bwt ~ Hwt, data = cats)
abline(lm_cats)
```

What does $\hat{\beta}_1$ tell us about how height affects weight?

------------------------------------------------------------------------

## Estimating $\sigma^2$ and $\sigma$

**Error sum of squares (SSE):**

$$
\text{SSE} = \sum (y_i - \hat{y}_i)^2
$$

**Estimate of variance:**

$$
\hat{\sigma}^2 = \frac{\text{SSE}}{n - 2}
$$

Used for inference and standard errors of slope/intercept.

------------------------------------------------------------------------

## Coefficient of Determination $R^2$

> Measures proportion of variance in $y$ explained by $x$.

$$
R^2 = 1 - \frac{\text{SSE}}{\text{SST}}, \quad \text{SST} = \sum (y_i - \bar{y})^2
$$

::: callout-warning
Beware! A high $R^2$ does **not** imply causation or even a good model
fit beyond the range of data.
:::


------------------------------------------------------------------------

## Summary

-   Simple linear regression models $E(Y|x) = \beta_0 + \beta_1 x$
-   Parameters estimated via **least squares**
-   Variability estimated via residual sum of squares (SSE)
-   $R^2$ quantifies model fit
-   Use `lm()`, `summary()`, and `anova()` in R to implement

:arrow_right: Next time: Inference for regression parameters and
prediction intervals
